What is Latency?

Latency is the time taken to process ONE request from start to finish.

ğŸ“Œ Measured in:

milliseconds (ms)

seconds (s)

Example

User clicks â€œLoginâ€

Server responds in 200 ms

ğŸ‘‰ Latency = 200 ms

Real-Life Analogy (Simple but Accurate)

You order coffee â˜•

Time from ordering â†’ receiving coffee = latency

Even if only one person is ordering, latency still exists.

ğŸš€ What is Throughput?

Throughput is the number of requests processed per unit time.

ğŸ“Œ Measured in:

requests per second (RPS)

transactions per second (TPS)

Example

Server handles 5,000 requests per second

ğŸ‘‰ Throughput = 5000 RPS

Real-Life Analogy

Coffee shop serves 100 customers per hour

Thatâ€™s throughput

Even if each coffee takes time, throughput depends on volume, not speed of one order.

ğŸ” Latency vs Throughput â€” Side-by-Side
Feature	Latency	Throughput
Measures	Time per request	Requests per time
Unit	ms / s	RPS / TPS
Focus	Speed	Capacity
Affected by	Network, DB, code	CPU, threads, scaling
User feels	Yes	Indirectly
âš ï¸ Important Truth (Read This Twice)

You can have:

âœ… Low Latency + âŒ Low Throughput

Very fast response

But crashes under load

Example:

A single powerful server serving 10 users very fast

âŒ High Latency + âœ… High Throughput

Handles massive traffic

Individual requests are slow

Example:

Batch processing systems

âœ… Low Latency + âœ… High Throughput (Ideal)

Fast responses

Scales under heavy load

This is what big tech aims for.

ğŸ§© How Latency and Throughput Are Related

They are related but not inversely proportional.

Key relationship:

As load increases, latency usually increases

Throughput increases until a saturation point

After saturation â†’ latency explodes, throughput drops

ğŸ“Œ This is called system overload

ğŸš§ What Increases Latency?

Network delays

Database queries

Disk I/O

Lock contention

Synchronous calls

Cold starts

Rule:

Every blocking operation adds latency.

ğŸš§ What Limits Throughput?

CPU cores

Memory

Thread pool size

Database connections

Locking & contention

I/O bandwidth

Rule:

Throughput is limited by the slowest shared resource.

âš™ï¸ Improving Latency vs Improving Throughput
To Reduce Latency

Caching

Faster algorithms

CDN usage

Reduce network hops

Async I/O

To Increase Throughput

Horizontal scaling

Load balancing

Stateless services

Queue-based processing

Batching requests

ğŸ¯ Interview Gold (Say This, Not Buzzwords)

If interviewer asks:

â€œWhatâ€™s more important: latency or throughput?â€

Correct answer:

Depends on the system use-case.

Examples:

Trading systems â†’ ultra-low latency

Video streaming â†’ high throughput

Chat apps â†’ balance of both

Batch analytics â†’ throughput > latency

âŒ Common Beginner Mistakes (Be Honest)

Confusing response time with throughput

Thinking AWS automatically fixes both

Ignoring database as bottleneck

Not measuring performance

If you donâ€™t measure:

Youâ€™re guessing, not engineering.